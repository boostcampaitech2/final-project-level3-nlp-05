{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Union, Optional, Tuple, List, Set\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import PretrainedConfig, PreTrainedTokenizerFast\n",
    "from transformers.models.bart.modeling_bart import BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "AIHub에서 다운 받은 [문서요약 텍스트 데이터셋](https://aihub.or.kr/aidata/8054)을 불러옵니다. \n",
    "\n",
    "아래의 `dataset_file`에 `.jsonl` 확장자 형태로 된 파일경로를 입력하면 json으로 불러오게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>abstractive</th>\n",
       "      <th>extractive</th>\n",
       "      <th>article_original</th>\n",
       "      <th>media</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>353465974</td>\n",
       "      <td>충주시는 민간보조사업의 증가와 보조금 집행관리에 대한 부당 행위가 증가함에따라 15...</td>\n",
       "      <td>[2, 3, 5]</td>\n",
       "      <td>[보조금 집행 위법행위·지적사례 늘어, 특별감사반, 2017~2018년 축제 점검,...</td>\n",
       "      <td>충청투데이</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>366398381</td>\n",
       "      <td>국무조정실은 8일 오후 대전시청에서 '대전지역 규제혁신 현장간담회'를 열고 대전과 ...</td>\n",
       "      <td>[4, 6, 14]</td>\n",
       "      <td>[8일 대전시청에서 규제혁신 간담회, 도시개발 산업용지도 특화단지 지정가능, 국무조...</td>\n",
       "      <td>중도일보</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>360025161</td>\n",
       "      <td>중국 경제일간지 21세기경제보도는 중국 대형 생명보험사인 차이나라이프가 '차이나라이...</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[중국 경제일간지 21세기경제보도는 중국 대형 생명보험사인 차이나라이프(中國人壽)가...</td>\n",
       "      <td>내일신문</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>361884128</td>\n",
       "      <td>1일 대검찰청은 '조속한 검찰개혁 방안을 마련하라'는 문재인 대통령의 지시에 따라...</td>\n",
       "      <td>[4, 5, 3]</td>\n",
       "      <td>[전승표 기자, 대검, 文 지시에 발빠른 방안 마련 서울중앙지검 3곳 빼고 모두 폐...</td>\n",
       "      <td>기호일보</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>351452460</td>\n",
       "      <td>제주도가 민선 7기 출범과 함께 조직개편을 추진하면서 지난해 8월 공무원 정원을 2...</td>\n",
       "      <td>[6, 11, 7]</td>\n",
       "      <td>[제주도가 공무원 정원 102명 증원을 추진하고 있는 가운데 제주도청 조직 및 인력...</td>\n",
       "      <td>한라일보</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                        abstractive  extractive  \\\n",
       "0  353465974  충주시는 민간보조사업의 증가와 보조금 집행관리에 대한 부당 행위가 증가함에따라 15...   [2, 3, 5]   \n",
       "1  366398381  국무조정실은 8일 오후 대전시청에서 '대전지역 규제혁신 현장간담회'를 열고 대전과 ...  [4, 6, 14]   \n",
       "2  360025161  중국 경제일간지 21세기경제보도는 중국 대형 생명보험사인 차이나라이프가 '차이나라이...   [0, 1, 2]   \n",
       "3  361884128   1일 대검찰청은 '조속한 검찰개혁 방안을 마련하라'는 문재인 대통령의 지시에 따라...   [4, 5, 3]   \n",
       "4  351452460  제주도가 민선 7기 출범과 함께 조직개편을 추진하면서 지난해 8월 공무원 정원을 2...  [6, 11, 7]   \n",
       "\n",
       "                                    article_original  media  \n",
       "0  [보조금 집행 위법행위·지적사례 늘어, 특별감사반, 2017~2018년 축제 점검,...  충청투데이  \n",
       "1  [8일 대전시청에서 규제혁신 간담회, 도시개발 산업용지도 특화단지 지정가능, 국무조...   중도일보  \n",
       "2  [중국 경제일간지 21세기경제보도는 중국 대형 생명보험사인 차이나라이프(中國人壽)가...   내일신문  \n",
       "3  [전승표 기자, 대검, 文 지시에 발빠른 방안 마련 서울중앙지검 3곳 빼고 모두 폐...   기호일보  \n",
       "4  [제주도가 공무원 정원 102명 증원을 추진하고 있는 가운데 제주도청 조직 및 인력...   한라일보  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_file = \"sample_data/labeled_data/newspaper/sample.jsonl\"\n",
    "\n",
    "with open(dataset_file, \"r\") as json_files:\n",
    "    json_list = list(json_files)\n",
    "\n",
    "raw_data = []\n",
    "for json_str in json_list:\n",
    "    result = json.loads(json_str)\n",
    "    raw_data.append(result)\n",
    "\n",
    "dataset_df = pd.DataFrame(raw_data)\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`extractive`와 `abstractive`의 두 개의 column이 존재하는 것을 볼 수 있는데,\n",
    "\n",
    "`extractive`는 `article_original`의 문장들 중 문장 요약에 중요한 3개의 문장의 index를 의미합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 5]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[0][\"extractive\"]\n",
    "# indicies of extractive summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['보조금 집행 위법행위·지적사례 늘어',\n",
       " '특별감사반, 2017~2018년 축제 점검',\n",
       " '충주시가 민간에게 지원되는 보조사업의 대형축제와 관련해 선정·집행·정산 등 운영실태 전반에 대한 자체 감사를 실시할 계획이라고 밝혔다.',\n",
       " '이는 최근 민간보조사업의 증가와 더불어 보조금 집행관리에 대한 위법 부당 행위와 지적사례가 지속적으로 증가함에 따라, 감사를 통해 취약요인을 점검해 올바른 보조금 사용 풍토를 정착시키겠다는 취지다.',\n",
       " '시는 감사담당관실과 기획예산과 보조금 관련 주무관으로 특별감사반을 편성해 2017년부터 2018년까지 집행된 축제성 보조금 집행에 대한 철저한 점검과 감사를 통해 부정 수급 및 부정 집행이 확인되면 엄정한 조치를 취할 방침이다.',\n",
       " '시는 지난 15일부터 25일까지 10일간의 사전감사를 통해 보조금 실태를 파악한 후, 8월15일까지 세부감사를 진행할 예정이라고 전했다.',\n",
       " '축제성 관련 부정수급 유형을 보면 허위·기타 부정한 방법으로 보조금 신청, 사업 실적을 부풀려 보조금을 횡령·편취, 보조금 교부 목적과 다른 용도로 집행, 보조금으로 취득한 재산에 대해 지자체장의 승인없이 임의 처분 등이 해당된다.',\n",
       " \"시는 불법보조금 근절과 효율적인 점검 및 적극적인 시민관심을 유도하기 위해 '지방보조금 부정수급 신고센터(☏850-5031)'를 설치 운영하고 있다.\",\n",
       " '지방보조금 부정수급 신고 시 직접방문 및 국민신문고(www.epeople.or.kr), 충주시홈페이지(www.chungju.or.kr)를 통해 접수하면 되고, 신고취지와 이유를 기재하고 부정행위와 관련한 증거자료를 제시하면 된다.',\n",
       " '단, 익명 신고는 접수치 않는다.',\n",
       " '시 관계자는 \"이번 자체 점검 및 감사를 통해 축제보조금이 제대로 쓰이는지에 대한 반성과 함께 보조금 집행의 투명성 및 행정의 신뢰성을 확보하는데 최선을 다하겠다\"고 말했다.',\n",
       " '한편. 시는 감사 및 예산부서 합동으로 컨설팅 위주의 상반기 보조금 특정감사(1월10일~20일)를 실시해 주의 11건, 시정 6건, 권고 1건을 자체 적발하고 조치한 바 있다.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[0][\"article_original\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['충주시가 민간에게 지원되는 보조사업의 대형축제와 관련해 선정·집행·정산 등 운영실태 전반에 대한 자체 감사를 실시할 계획이라고 밝혔다.',\n",
       " '이는 최근 민간보조사업의 증가와 더불어 보조금 집행관리에 대한 위법 부당 행위와 지적사례가 지속적으로 증가함에 따라, 감사를 통해 취약요인을 점검해 올바른 보조금 사용 풍토를 정착시키겠다는 취지다.',\n",
       " '시는 지난 15일부터 25일까지 10일간의 사전감사를 통해 보조금 실태를 파악한 후, 8월15일까지 세부감사를 진행할 예정이라고 전했다.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "[raw_data[idx][\"article_original\"][i] for i in raw_data[idx][\"extractive\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 최종 프로젝트에서는 extractive와 abstractive를 단일 모델을 이용하여 모델링을 하고자 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pretrained Model\n",
    "\n",
    "기본 베이스라인으로는 HuggingFace의 gogamza/kobart-summarization 을 사용합니다. \n",
    "\n",
    "해당 모델을 불러와서 모델의 구조를 출력하면 아래와 같습니다.\n",
    "\n",
    "```python\n",
    "BartForConditionalGeneration(\n",
    "    (model): BartModel(\n",
    "        (shared): Embedding(30000, 768. padding_idx=3)\n",
    "        (encoder): BartEncoder(\n",
    "            (embed_tokens): Embedding(30000, 768, padding_idx=3)\n",
    "            (embed_positions): BartLearnedPositionalEmbedding(1028, 768, padding_idx=3)\n",
    "            (layers): ModuleList(\n",
    "                (0): BartEncoderLayer( ... ) ...\n",
    "                (5): BartEncoderLayer( ... ) ...\n",
    "            )\n",
    "            (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "        )\n",
    "        (decoder): BartDecoder(\n",
    "            (embed_tokens): Embedding(30000, 768, padding_idx=3)\n",
    "            (embed_positions): BartLearnedPositionalEmbedding(1028, 768, padding_idx=3)\n",
    "            (layers): ModuleList(\n",
    "                (0): BartDecoderLayer( ... ) ...\n",
    "                (5): BartDecoderLayer( ... ) ...\n",
    "            )\n",
    "            (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "    )\n",
    "    (lm_head): Linear(in_features=768, out_features=30000, bias=False)\n",
    ")\n",
    "```\n",
    "\n",
    "## Train\n",
    "\n",
    "단일의 배치에 대해서 extraction-based summarization과 generation-based summarization 두 번의 train step이 동작하게 됩니다.   \n",
    "\n",
    "## Inference\n",
    "\n",
    "위 모델 구조에서 `model.encoder`의 `last_hidden_states`를 이용해서 **extraction-based summarization**을 1차적으로 거친 뒤에,   \n",
    "이를 다시 `model`에 넣고 `model.decoder`에서 **generation-based summarization**을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gogamza/kobart-summarization\"\n",
    "\n",
    "config = PretrainedConfig.from_pretrained(MODEL_NAME)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "model = BartForConditionalGeneration.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PretrainedConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"transformers_version\": \"4.3.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder -> Classification 훈련\n",
    "\n",
    "예시 문장을 이용해서 진행하면 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryDataset(Dataset):\n",
    "    def __init__(self, path: str, tokenizer: PreTrainedTokenizerFast, sep_token_id: int):\n",
    "        self.path = path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sep_token_id = sep_token_id\n",
    "\n",
    "        with open(path, \"r\") as json_files:\n",
    "            json_list = list(json_files)\n",
    "\n",
    "        raw_data = []\n",
    "        for json_str in json_list:\n",
    "            result = json.loads(json_str)\n",
    "            raw_data.append(result)\n",
    "\n",
    "        self.raw_data = raw_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_sentences = self.raw_data[idx][\"article_original\"]\n",
    "        target_sentence = self.raw_data[idx][\"abstractive\"]\n",
    "        target_ids = self.raw_data[idx][\"extractive\"]\n",
    "\n",
    "        input_ids = []\n",
    "        bos_positions = []\n",
    "\n",
    "        for sentence in input_sentences:\n",
    "            bos_positions.append(len(input_ids))\n",
    "            input_ids.append(self.tokenizer.bos_token_id)\n",
    "            input_ids.extend(self.tokenizer.encode(sentence))\n",
    "        input_ids.append(self.tokenizer.eos_token_id)\n",
    "\n",
    "        answer_positions = [bos_positions[i] for i in target_ids]\n",
    "\n",
    "        labels = [self.tokenizer.bos_token_id] + self.tokenizer.encode(target_sentence) + [self.tokenizer.eos_token_id]\n",
    "\n",
    "        return {\"input_ids\": torch.tensor(input_ids),\n",
    "                \"attention_mask\": torch.ones(len(input_ids)),\n",
    "                \"labels\": torch.tensor(labels),\n",
    "                \"bos_positions\": torch.tensor(bos_positions),\n",
    "                \"answer_positions\": torch.tensor(answer_positions),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels', 'bos_positions', 'answer_positions'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_file = \"sample_data/labeled_data/newspaper/sample.jsonl\"\n",
    "dataset = SummaryDataset(dataset_file, tokenizer, tokenizer.bos_token_id)\n",
    "dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, 16, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 10\n",
    "\n",
    "input_sentences = raw_data[idx][\"article_original\"]\n",
    "target_sentence = raw_data[idx][\"abstractive\"]\n",
    "target_ids = raw_data[idx][\"extractive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "bos_positions = []\n",
    "\n",
    "for sentence in input_sentences:\n",
    "    bos_positions.append(len(input_ids))\n",
    "    input_ids.append(tokenizer.bos_token_id)\n",
    "    input_ids.extend(tokenizer.encode(sentence))\n",
    "input_ids.append(tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 16193, 10213, 10301, 12034, 10301, 15487, 17289, 240, 1]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[0:5] + input_ids[-5:]\n",
    "# 각 문장의 시작에는 0이 붙고, 각 문장의 끝에는 1이 붙습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 19: 0,\n",
       " 57: 0,\n",
       " 85: 0,\n",
       " 136: 0,\n",
       " 160: 0,\n",
       " 180: 0,\n",
       " 199: 0,\n",
       " 231: 0,\n",
       " 249: 0,\n",
       " 260: 0,\n",
       " 290: 0,\n",
       " 330: 0,\n",
       " 364: 0,\n",
       " 395: 0,\n",
       " 449: 0,\n",
       " 486: 0,\n",
       " 495: 0}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{p: input_ids[p] for p in bos_positions}\n",
    "# bos token이 삽입된 곳을 올바르게 트래킹하는 것을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 57, 449]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_positions = [bos_positions[i] for i in target_ids]\n",
    "answer_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_label = torch.zeros(len(input_ids))\n",
    "classification_label.index_fill_(0, torch.tensor(answer_positions), 1)\n",
    "classification_label\n",
    "# encoder output의 정답으로 사용할 label을 만들었습니다.\n",
    "# 위에서 answer_positions에 해당하는 3개의 값만 1이고, 나머지는 0을 갖습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-6.6001e-02,  1.1041e-02, -5.9069e-02,  ...,  2.0143e-02,\n",
       "           3.4490e-02, -7.9267e-02],\n",
       "         [ 1.6924e-01,  5.5691e-02, -4.9537e-02,  ..., -3.2821e-01,\n",
       "          -3.4019e-03, -2.1108e-01],\n",
       "         [ 6.5241e-01,  4.4865e-01, -2.7274e-01,  ..., -3.0237e-01,\n",
       "          -3.4908e-01,  1.1237e-01],\n",
       "         ...,\n",
       "         [-3.9484e-02, -9.8896e-02, -6.3180e-02,  ..., -1.0830e-01,\n",
       "           6.8898e-02, -5.7976e-04],\n",
       "         [ 1.1946e-01, -2.0024e-01,  4.8521e-01,  ...,  3.4767e-01,\n",
       "          -2.3743e-01, -8.7730e-02],\n",
       "         [-8.8896e-02,  3.3258e-02,  1.0648e-02,  ...,  2.0099e-02,\n",
       "          -8.2142e-03, -2.5698e-03]]], grad_fn=<NativeLayerNormBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_out = model.model.encoder(torch.tensor([input_ids]))\n",
    "encoder_out\n",
    "# encoder의 첫번째 Output은 last_hidden_state입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = nn.Linear(config.d_model, 1)\n",
    "# classification layer로 이용할 linear layer를 하나 만듭니다.\n",
    "# 이 뒤에 LSTM 등 다양한 classification layer head 구조를 활용할 수 있을 것이라고 생각됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 513])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = fc(encoder_out[0])\n",
    "logit = logit.squeeze(-1)\n",
    "logit.shape\n",
    "# [B, L]의 shape를 갖습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.ones(len(input_ids))\n",
    "mask = mask.index_fill(0, torch.tensor(bos_positions), 0)\n",
    "mask\n",
    "# logit = logit.masked_fill(mask, -1e9)\n",
    "# classification에 관여하지 않는 bos_token이 아닌 나머지 logit 값들을 `-1e9`라는 매우 큰 값으로 설정하여\n",
    "# 실제로 확률값이 0이 되도록 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = logit.masked_fill(mask == 1, -1e9)\n",
    "prob = torch.sigmoid(logit)\n",
    "# logit 값에 sigmoid 함수를 취하면 확률값이 됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f327df75c40>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(prob.detach().numpy(), aspect=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 85, 231, 136]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argsort(prob, descending=True)[:, 0:3]\n",
    "# top-k sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss(reduction=\"none\")\n",
    "# 일단, reduction을 \"none\"으로 하고 모델의 loss가 각각에 대해 올바르게 계산되는지를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6354, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.7416, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.6341, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.7910, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.7702, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7686, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.7494, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.7485, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7850, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6148, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7282,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.7257, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7232, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.7393, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7272,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6510,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.7378, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.7401, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "       grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(prob, classification_label.unsqueeze(0))\n",
    "# 현재는 batch가 아니라서 unsqueeze를 해줘야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.0111, grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.BCELoss(reduction=\"sum\")\n",
    "loss = criterion(prob, classification_label.unsqueeze(0))\n",
    "loss\n",
    "# reduction을 \"sum\"으로 하면 각 batch별로 단일의 loss 값이 scalar 값으로 도출됩니다.\n",
    "# 이 loss 값을 바탕으로 backward가 일어납니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder -> Decoder 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"맥라렌이 영국에서 새로운 '600LT 스파이더'를 공개했다. 맥라렌이 2019년 새롭게 선보이는 모델은 전설이 맥라렌 롱테일의 명성을 이어갈 5번째 롱테일이다. 맥라렌 600LT 스파이더는 쿠페와 마찬가지로 영국 워킹에 위치한 맥라렌 프로덕션 센터에서 수작업으로 조립되는 한정판이다.\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seleted_sentences = [input_sentences[idx] for idx in target_ids]\n",
    "# 0번 문장이 대체로 title로 추정되는데, 이를 추가할지 말지도 여기서 결정해야 할 것 같습니다.\n",
    "\n",
    "extracted_sentences = \" \".join(seleted_sentences)\n",
    "extracted_sentences\n",
    "# generation-based summary에 사용될 문장을 만들어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2, 16193, 10213, 10301, 12034, 15609, 15717, 13120, 11786, 17715,\n",
       "         16193, 10213, 10301, 14430,  9807, 11300, 18627, 14030, 14032, 20356,\n",
       "         14027, 14077, 10487, 14343, 19363, 13372, 14063,   253, 14079,   275,\n",
       "           283, 14148, 19052,  9806, 15127, 14623, 15615,     1]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = [tokenizer.bos_token_id] + tokenizer.encode(extracted_sentences) + [tokenizer.eos_token_id]\n",
    "summary_ids = model.generate(torch.tensor([input_ids]), max_length=256, min_length=20, early_stopping=True, num_beams=20)\n",
    "summary_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"맥라렌이 영국 워킹에 위치한 맥라렌 프로덕션 센터에서 수작업으로 조립되는 한정판 '600LT 스파이더'를 공개했다.\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 요약 결과를 전체 문장을 넣었을 때와 비교해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"맥라렌이 맥라렌의 여섯 가지 LT 정신인 강력한 성능, 경량화 차체, 최적화된 공력 성능, 트랙 중심의 역동성, 드라이버와 완벽한 교감, 희소성 등을 아우르는 새로운 '600LT 스파이더'를 공개했다.\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = [tokenizer.bos_token_id] + tokenizer.encode(\" \".join(input_sentences)) + [tokenizer.eos_token_id]\n",
    "summary_ids = model.generate(torch.tensor([input_ids]), max_length=256, min_length=20, early_stopping=True, num_beams=20)\n",
    "tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
